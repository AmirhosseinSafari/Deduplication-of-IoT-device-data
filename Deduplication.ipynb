{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Zone\n",
    "import os\n",
    "# import sys\n",
    "import random\n",
    "import nltk\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from Crypto.Cipher import AES\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenshtein Distance Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein Distance Algorithm\n",
    "\n",
    "def damerau_levenshtein_distance(string1, string2):\n",
    "    n1 = len(string1)\n",
    "    n2 = len(string2)\n",
    "    return _levenshtein_distance_matrix(string1, string2, True)[n1, n2]\n",
    "\n",
    "def get_ops(string1, string2, is_damerau=False):\n",
    "    i, j = _levenshtein_distance_matrix(string1, string2, is_damerau).shape\n",
    "    i -= 1\n",
    "    j -= 1\n",
    "    ops = list()\n",
    "    while i != -1 and j != -1:\n",
    "        if is_damerau:\n",
    "            if i > 1 and j > 1 and string1[i-1] == string2[j-2] and string1[i-2] == string2[j-1]:\n",
    "                if dist_matrix[i-2, j-2] < dist_matrix[i, j]:\n",
    "                    ops.insert(0, ('transpose', i - 1, i - 2))\n",
    "                    i -= 2\n",
    "                    j -= 2\n",
    "                    continue\n",
    "        index = np.argmin([dist_matrix[i-1, j-1], dist_matrix[i, j-1], dist_matrix[i-1, j]])\n",
    "        if index == 0:\n",
    "            if dist_matrix[i, j] > dist_matrix[i-1, j-1]:\n",
    "                ops.insert(0, ('replace', i - 1, j - 1))\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif index == 1:\n",
    "            ops.insert(0, ('insert', i - 1, j - 1))\n",
    "            j -= 1\n",
    "        elif index == 2:\n",
    "            ops.insert(0, ('delete', i - 1, i - 1))\n",
    "            i -= 1\n",
    "    return ops\n",
    "\n",
    "def execute_ops(ops, string1, string2):\n",
    "    strings = [string1]\n",
    "    string = list(string1)\n",
    "    shift = 0\n",
    "    for op in ops:\n",
    "        i, j = op[1], op[2]\n",
    "        if op[0] == 'delete':\n",
    "            del string[i + shift]\n",
    "            shift -= 1\n",
    "        elif op[0] == 'insert':\n",
    "            string.insert(i + shift + 1, string2[j])\n",
    "            shift += 1\n",
    "        elif op[0] == 'replace':\n",
    "            string[i + shift] = string2[j]\n",
    "        elif op[0] == 'transpose':\n",
    "            string[i + shift], string[j + shift] = string[j + shift], string[i + shift]\n",
    "        strings.append(''.join(string))\n",
    "    return strings\n",
    "#Levenshtein Distance \n",
    "def _levenshtein_distance_matrix(string1, string2, is_damerau=False):\n",
    "    n1 = len(string1)\n",
    "    n2 = len(string2)\n",
    "    d = np.zeros((n1 + 1, n2 + 1), dtype=int)\n",
    "    for i in range(n1 + 1):\n",
    "        d[i, 0] = i\n",
    "    for j in range(n2 + 1):\n",
    "        d[0, j] = j\n",
    "    for i in range(n1):\n",
    "        for j in range(n2):\n",
    "            if string1[i] == string2[j]:\n",
    "                cost = 0\n",
    "            else:\n",
    "                cost = 1\n",
    "            d[i+1, j+1] = min(d[i, j+1] + 1, # insert\n",
    "                              d[i+1, j] + 1, # delete\n",
    "                              d[i, j] + cost) # replace\n",
    "            if is_damerau:\n",
    "                if i > 0 and j > 0 and string1[i] == string2[j-1] and string1[i-1] == string2[j]:\n",
    "                    d[i+1, j+1] = min(d[i+1, j+1], d[i-1, j-1] + cost) # transpose\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bit to megabyte\n",
    "def convert_Bit_to_Megabyte(bit):\n",
    "    Megabyte = bit/(1024*1024*8)\n",
    "    return Megabyte\n",
    "def CalCharsbit(word):\n",
    "    wordcount = len(word)*8\n",
    "    return wordcount\n",
    "# storage of a list\n",
    "def calstorageofList(alist):\n",
    "    strforlist=0\n",
    "    for i in alist:\n",
    "        strforlist += CalCharsbit(i)\n",
    "    return strforlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the required paramerters from args.json file\n",
    "\n",
    "with open('args.json', 'r') as json_file:\n",
    "    # Parse the JSON data into a Python dictionary\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "threshold = parameters[\"threshold\"]\n",
    "Memorysize = parameters[\"Memorysize\"]\n",
    "Hashsize = parameters[\"Hashsize\"]\n",
    "encSize = parameters[\"encSize\"]\n",
    "thestartrow = parameters[\"thestartrow\"]\n",
    "ifdemaru= False\n",
    "AES_key = parameters[\"AES_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "data config: threshold:  6  DL(F/T):  False  Memorysize:  0.005  MB   Hash size:  8  Encryption size  128\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "print(\"data config: threshold: \",threshold,\" DL(F/T): \",ifdemaru ,\" Memorysize: \", Memorysize, \" MB \",\" Hash size: \",Hashsize, \" Encryption size \", encSize)\n",
    "print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from dataset\n",
    "data = [json.loads(line)\n",
    "        for line in open('train.json', 'r', encoding='utf-8')]\n",
    "\n",
    "DataSet=[]\n",
    "for item in data:\n",
    "        DataSet.append(item['navigation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting size of dataset\n",
    "SampleSize = len(DataSet)\n",
    "\n",
    "# Raw data storage (char by char calculation)\n",
    "liststr = []\n",
    "for item in DataSet:\n",
    "    strforlist = calstorageofList(item)\n",
    "    liststr.append(strforlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "#nltk.download('punkt')\n",
    "def tokenize(sampleEmail):\n",
    "    words = nltk.tokenize.word_tokenize(str(sampleEmail))\n",
    "    return words\n",
    "    \n",
    "#gives the list of tokenized elements\n",
    "tokenized = []\n",
    "for entry in DataSet:\n",
    "    tokenized.append(tokenize(entry))\n",
    "\n",
    "NumofTokenizedWords = 0\n",
    "for i in tokenized:\n",
    "    NumofTokenizedWords += len(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Bases & devs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_binary(astring):\n",
    "    return ''.join(format(ord(i), 'b') for i in astring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crc_remainder(input_string, polynomial_bitstring, initial_filler):\n",
    "    \"\"\"Calculate the CRC remainder of a string of bits using a chosen polynomial.\n",
    "    initial_filler should be '1' or '0'.\n",
    "    \"\"\"\n",
    "    input_bitstring = string_to_binary(input_string)\n",
    "    polynomial_bitstring = polynomial_bitstring.lstrip('0')\n",
    "    len_input = len(input_bitstring)\n",
    "    initial_padding = (len(polynomial_bitstring) - 1) * initial_filler\n",
    "    input_padded_array = list(input_bitstring + initial_padding)\n",
    "    while '1' in input_padded_array[:len_input]:\n",
    "        cur_shift = input_padded_array.index('1')\n",
    "        for i in range(len(polynomial_bitstring)):\n",
    "            input_padded_array[cur_shift + i] \\\n",
    "            = str(int(polynomial_bitstring[i] != input_padded_array[cur_shift + i]))\n",
    "    return ''.join(input_padded_array)[len_input:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_random_polynomial(bitsNumber):\n",
    "    # Gets number of bits (N) and returns a N-bit random binary string\n",
    "    binaryRand=\"\"\n",
    "    for i in range(bitsNumber):\n",
    "        randBit = random.randint(0, 1)\n",
    "        binaryRand += str(randBit)\n",
    "    \n",
    "    binaryRand = '1' + binaryRand\n",
    "\n",
    "    return binaryRand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encryption_AES(text):\n",
    "    cipher = AES.new(AES_key.encode(\"utf8\"), AES.MODE_EAX)\n",
    "    nonce = cipher.nonce\n",
    "    cipherbyte, tag = cipher.encrypt_and_digest(text.encode(\"utf8\"))\n",
    "\n",
    "    return cipherbyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_base64(byte_cipher):\n",
    "    return base64.b64encode(byte_cipher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sending_crc_devs(devs, crc):\n",
    "    message = str((devs, crc))\n",
    "    sending_commmand = f\"mosquitto_pub -h test.mosquitto.org -t /zvrqrh -m \\\"{message}\\\"\"\n",
    "    os.system(sending_commmand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sending_crc_devs_encyption(devs, crc, encrypted_base64):\n",
    "    message = str((devs, crc, encrypted_base64))\n",
    "    sending_commmand = f\"mosquitto_pub -h test.mosquitto.org -t /zvrqrh -m \\\"{message}\\\"\"\n",
    "    os.system(sending_commmand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storages and variables \n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "numberofDels=0\n",
    "numberofIns=0\n",
    "numberofRep=0\n",
    "numberoftrans=0\n",
    "\n",
    "polynomial_bitstring=binary_random_polynomial(Hashsize)\n",
    "\n",
    "FinalBase=\"\"\n",
    "Base_Freq={}\n",
    "localBases=[]\n",
    "HashListSenttoServer=[]\n",
    "SendServerencryptedbased=0\n",
    "notmergedBases=[]\n",
    "mergedBases=[]\n",
    "DeviationsStorage=[]\n",
    "numodfDuplicateSentBases=0\n",
    "localstorageHASHbased=0\n",
    "mergedduplicates=0\n",
    "mergedBasesSize=0\n",
    "notmergedBasesSize=0\n",
    "Sendmorehashes=0\n",
    "pointertonewlyadded=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Bases & devs\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "for singleEntry in tokenized:   \n",
    "    for word in singleEntry:\n",
    "        devsize=0\n",
    "        ops = []\n",
    "        #ectract the base started\n",
    "        if len(word)*8 <= int(encSize):\n",
    "            #extract the base of the word\n",
    "            extractedBase = wordnet_lemmatizer.lemmatize(word)    \n",
    "\n",
    "            #find the distance and get the operations for converting\n",
    "            dist_matrix = _levenshtein_distance_matrix(extractedBase, word, is_damerau=ifdemaru)\n",
    "            ops = get_ops(extractedBase, word, is_damerau=ifdemaru)\n",
    "\n",
    "            numofedits = len(ops)\n",
    "\n",
    "            #if less than threshold, define a deviation\n",
    "            # ops are the final operations and they should be sent\n",
    "            if numofedits>0 and numofedits<int(threshold):\n",
    "                FinalBase = extractedBase\n",
    "                for i in range(len(ops)):\n",
    "                    if ops[i][0]=='delete':\n",
    "                        devsize+=(2+8+math.ceil(math.log2(len(extractedBase))))\n",
    "                        numberofDels+=1\n",
    "                    elif ops[i][0]=='replace':\n",
    "                        devsize+=(2+8+math.ceil(math.log2(len(extractedBase))))\n",
    "                        numberofRep+=1\n",
    "                    elif ops[i][0]=='insert':\n",
    "                        devsize+=(2+8+math.ceil(math.log2(len(extractedBase))))\n",
    "                        numberofIns+=1\n",
    "                    elif ops[i][0]=='transpose':\n",
    "                        devsize+=(2+math.ceil(math.log2(len(extractedBase))))\n",
    "                        numberoftrans+=1\n",
    "                DeviationsStorage.append(devsize)\n",
    "\n",
    "            # if dist=0 OR dist >= threshold the final base would be the word itself \n",
    "            else:\n",
    "                FinalBase=word\n",
    "\n",
    "        #else if the length of the word is more than 15 (2^4 = 2^7 / 8)\n",
    "        else:\n",
    "            FinalBase=word\n",
    "        #ectracting the base ends here\n",
    "\n",
    "        #check if its already available in local storage - if yes only the hash is sent to the server\n",
    "        if FinalBase in localBases:\n",
    "            #increase the number of encrypted bases that are sent\n",
    "            numodfDuplicateSentBases += 1\n",
    "            \n",
    "            #send the hash(base)\n",
    "            #if hashing is used as an ID (e.g. in md5, 128 bit is rquried) \n",
    "            crc = crc_remainder(FinalBase, polynomial_bitstring, '0')\n",
    "            \n",
    "            # Sending: (ops, hash)\n",
    "            sending_crc_devs(ops, crc)\n",
    "            HashListSenttoServer.append(crc)\n",
    "\n",
    "            #increase the frequency of the base \n",
    "            if FinalBase in Base_Freq:\n",
    "                Base_Freq[FinalBase] += 1\n",
    "            else:\n",
    "                Base_Freq[FinalBase] = 1\n",
    "\n",
    "        #if its not avialbe in the local storage -> then we should check if it still has some places left or not!        \n",
    "        else:\n",
    "            #calculate the storage needed for storing a hash based on given hash function\n",
    "            OnehashsizeinMB = convert_Bit_to_Megabyte(int(Hashsize))\n",
    "            #check if localstorage is not full yet\n",
    "            if convert_Bit_to_Megabyte(localstorageHASHbased) <= (float(Memorysize)-OnehashsizeinMB):\n",
    "\n",
    "                #the raw value appended just for simplicity of comparing\n",
    "                localBases.append(FinalBase)\n",
    "    \n",
    "                #add the hash value size to bases\n",
    "                # ToDo[tag.2]: we should calculate hashes, create a storage(dict) for local hashes \n",
    "                # and store hashes there\n",
    "                crc = crc_remainder(FinalBase, polynomial_bitstring, '0')\n",
    "                localstorageHASHbased+=int(Hashsize)\n",
    "\n",
    "                #increase the frequency of the base\n",
    "                if FinalBase in Base_Freq:\n",
    "                    Base_Freq[FinalBase] +=1\n",
    "                else:\n",
    "                    Base_Freq[FinalBase] =1\n",
    "\n",
    "                # ToDo[tag.1]: We should really send it to server (publisher)\n",
    "                #send server the ecnrypted(base) - the size is based on the encryption method\n",
    "                # Encrypting the Final base and converting it to Base64 for sending it\n",
    "                encrypted_text = encryption_AES(FinalBase)\n",
    "                encrypted_base64_text = to_base64(encrypted_text)\n",
    "                \n",
    "                \n",
    "                # Ask: should we do the server(publisher) side procedure?\n",
    "                sending_crc_devs_encyption(ops, crc, encrypted_base64_text)\n",
    "                Sendmorehashes+=int(Hashsize)\n",
    "                SendServerencryptedbased += int(encSize)\n",
    "\n",
    "\n",
    "                #server side storage simulation\n",
    "                if FinalBase not in mergedBases: \n",
    "                    mergedBasesSize+= int(encSize)\n",
    "                    mergedBases.append(FinalBase)\n",
    "                    pointertonewlyadded+=1\n",
    "                    \n",
    "                else:\n",
    "                    mergedduplicates+=1\n",
    "\n",
    "\n",
    "            #if the storage is full \n",
    "            else: \n",
    "                #extract the one with minimum frequency\n",
    "                min_freq =  min(Base_Freq, key=Base_Freq.get)\n",
    "                localBases.remove(min_freq)\n",
    "                Base_Freq.pop(min_freq)\n",
    "                #set the frequency to zero\n",
    "                Base_Freq[FinalBase] = 0\n",
    "                localBases.append(FinalBase)\n",
    "                \n",
    "                #send server the ecnrypted(base) - the size is based on the encryption method\n",
    "                encrypted_text = encryption_AES(FinalBase)\n",
    "                encrypted_base64_text = to_base64(encrypted_text)\n",
    "\n",
    "                # ToDo[tag.2]: we should calculate hashes, create a storage(dict) for local hashes \n",
    "                # and store hashes there \n",
    "                # (Ask: here kiyana stores the FinalBase and check with it not hash (CRC) of it, \n",
    "                # should we compare it with the CRC?)\n",
    "                crc = crc_remainder(FinalBase, polynomial_bitstring, '0')\n",
    "                \n",
    "                sending_crc_devs_encyption(ops, crc, encrypted_base64_text)\n",
    "                Sendmorehashes+=int(Hashsize)\n",
    "                SendServerencryptedbased+= int(encSize)\n",
    "                \n",
    "                #server side storage simulation\n",
    "                # Ask: should we do the server(publisher) side procedure?\n",
    "                if FinalBase not in mergedBases: \n",
    "                    pointertonewlyadded+=1\n",
    "                    mergedBases.append(FinalBase)\n",
    "                    mergedBasesSize+= int(encSize)\n",
    "                else:\n",
    "                    mergedduplicates+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#server Storage pointer based\n",
    "totaldups= numodfDuplicateSentBases+mergedduplicates\n",
    "\n",
    "ServerStorage=convert_Bit_to_Megabyte(sum(DeviationsStorage)+mergedBasesSize+(totaldups*math.log2(len(mergedBases)))+(int(Hashsize)*len(mergedBases)))\n",
    "\n",
    "pointerofnewlyaddedsize=convert_Bit_to_Megabyte(pointertonewlyadded*math.log2(len(mergedBases)))\n",
    "ServerStoragewithpointer=convert_Bit_to_Megabyte(sum(DeviationsStorage)+mergedBasesSize+(totaldups*math.log2(len(mergedBases)))+pointertonewlyadded*math.log2(len(mergedBases))+ (int(Hashsize)*len(mergedBases)))\n",
    "\n",
    "results = {'Memory size ': str(Memorysize),\n",
    "                  'SampleSize ': str(SampleSize),\n",
    "                  'raw  storage (MB) ': str(convert_Bit_to_Megabyte(sum(liststr))),\n",
    "                  'number of tokenized words': str(NumofTokenizedWords),\n",
    "                  'hash size': str(Hashsize),\n",
    "                  'threshold': str(threshold),\n",
    "                  'enc size': str(encSize),\n",
    "                  'ifdemaru': str(ifdemaru),\n",
    "                  'local storage (MB)': str(convert_Bit_to_Megabyte(localstorageHASHbased)),\n",
    "                  'ServerStorage (MB)': str(ServerStorage),\n",
    "                  'pointers to newly (MB)': str(pointerofnewlyaddedsize),\n",
    "                  'Total ServerStoragewithpointer (MB)': str(ServerStoragewithpointer),\n",
    "                  'encrypted Sent bases (MB)': str(convert_Bit_to_Megabyte(SendServerencryptedbased)),\n",
    "                  'encrypted deviations (MB)': str(convert_Bit_to_Megabyte(sum(DeviationsStorage))),\n",
    "                  'Send Server Hash  (MB)': str(convert_Bit_to_Megabyte(len(HashListSenttoServer) * Hashsize)),\n",
    "                  'Sendmorehashes (MB)': str(convert_Bit_to_Megabyte(Sendmorehashes)),\n",
    "                  'size of merged stored bases (MB)': str(convert_Bit_to_Megabyte(mergedBasesSize)),\n",
    "                  'numodfDuplicateSentBases (hashes)': str(numodfDuplicateSentBases),                  \n",
    "                  'number of merged duplicates (final bases)': str(mergedduplicates),\n",
    "                  'number of merged bases': str(len(mergedBases)),\n",
    "                  'len(deviationList)': str(len(DeviationsStorage)),\n",
    "                  'tot pointer storage for merged': str(convert_Bit_to_Megabyte(totaldups*math.log2(len(mergedBases)))),\n",
    "                  }\n",
    "\n",
    "path = 'touchdownResultsJuly_1402_7_3'\n",
    "with open(path, 'w') as f:\n",
    "     f.write(json.dumps(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to touchdownResultsJuly_1402_7_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "#server Storage pointer based\n",
    "totaldups= numodfDuplicateSentBases+mergedduplicates\n",
    "\n",
    "ServerStorage=convert_Bit_to_Megabyte(sum(DeviationsStorage)+mergedBasesSize+(totaldups*math.log2(len(mergedBases)))+(int(Hashsize)*len(mergedBases)))\n",
    "\n",
    "pointerofnewlyaddedsize=convert_Bit_to_Megabyte(pointertonewlyadded*math.log2(len(mergedBases)))\n",
    "ServerStoragewithpointer=convert_Bit_to_Megabyte(sum(DeviationsStorage)+mergedBasesSize+(totaldups*math.log2(len(mergedBases)))+pointertonewlyadded*math.log2(len(mergedBases))+ (int(Hashsize)*len(mergedBases)))\n",
    "\n",
    "df = pd.DataFrame({'Memory size ': [str(Memorysize)],\n",
    "                  'SampleSize ': [str(SampleSize)],\n",
    "                  'raw  storage (MB) ': [str(convert_Bit_to_Megabyte(sum(liststr)))],\n",
    "                  'number of tokenized words': [str(NumofTokenizedWords)],\n",
    "                  'hash size': [str(Hashsize)],\n",
    "                  'threshold': [str(threshold)],\n",
    "                  'enc size': [str(encSize)],\n",
    "                  'ifdemaru': [str(ifdemaru)],\n",
    "                  'local storage (MB)': [str(convert_Bit_to_Megabyte(localstorageHASHbased))],\n",
    "                  'ServerStorage (MB)': [str(ServerStorage)],\n",
    "                  'pointers to newly (MB)': [str(pointerofnewlyaddedsize)],\n",
    "                  'Total ServerStoragewithpointer (MB)': [str(ServerStoragewithpointer)],\n",
    "                  'encrypted Sent bases (MB)': [str(convert_Bit_to_Megabyte(SendServerencryptedbased))],\n",
    "                  'encrypted deviations (MB)': [str(convert_Bit_to_Megabyte(sum(DeviationsStorage)))],\n",
    "                  'Send Server Hash  (MB)': [str(convert_Bit_to_Megabyte(len(HashListSenttoServer) * Hashsize))],\n",
    "                  'Sendmorehashes (MB)': [str(convert_Bit_to_Megabyte(Sendmorehashes))],\n",
    "                  'size of merged stored bases (MB)': [str(convert_Bit_to_Megabyte(mergedBasesSize))],\n",
    "                  'numodfDuplicateSentBases (hashes)': [str(numodfDuplicateSentBases)],                  \n",
    "                  'number of merged duplicates (final bases)': [str(mergedduplicates)],\n",
    "                  'number of merged bases': [str(len(mergedBases))],\n",
    "                  'len(deviationList)': [str(len(DeviationsStorage))],\n",
    "                  'tot pointer storage for merged': [str(convert_Bit_to_Megabyte(totaldups*math.log2(len(mergedBases))))],\n",
    "                  })\n",
    "\n",
    "# Write the DataFrame to an Excel file\n",
    "excel_file_path = 'touchdownResultsJuly_1402_7_2.xlsx'\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully written to {excel_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
